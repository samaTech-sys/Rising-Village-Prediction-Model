{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c15ca7",
   "metadata": {},
   "source": [
    "## Trial-runs for model evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cb3a9ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e8efbe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\USER\\Desktop\\MLDefaults\\Rising-Village-Prediction-Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dfaada04",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"https://dagshub.com/samaTech-sys/Rising-Village-Prediction-Model.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"SamaTech-sys\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"27c279e0895284a35a6198b88076b93c13073cab\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c96a32",
   "metadata": {},
   "source": [
    "## Trial-runs for entity_config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d5a9219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path \n",
    "    model_path: Path \n",
    "    all_params: dict\n",
    "    metric_file_name: Path\n",
    "    target_column: str\n",
    "    mlflow_uri: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd6d6f",
   "metadata": {},
   "source": [
    "## Trial-runs for the ConfigarationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f663f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all project paths and modules necessary for project configurations \n",
    "from raisingVillage.constants import *\n",
    "from raisingVillage.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "78ad5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH, \n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        selected_schema_filepath = SELECTED_SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.selected_schema = read_yaml(selected_schema_filepath)\n",
    "       \n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation  # Should probably use model_evaluation section\n",
    "        data_splitting_config = self.config.data_splitting\n",
    "        model_training_config = self.config.model_training\n",
    "        \n",
    "        # Combine all parameters into one dictionary\n",
    "        all_params = {\n",
    "            \"tfidf_params\": self.params.model_training.TfidfVectorizer,\n",
    "            \"gb_params\": self.params.model_training.GradientBoostingClassifier\n",
    "        }\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            test_data_path=Path(data_splitting_config.test_set_path),\n",
    "            model_path=Path(model_training_config.model_path),\n",
    "            all_params=all_params,\n",
    "            metric_file_name=Path(config.metric_file_name),\n",
    "            target_column=self.selected_schema.TARGET,\n",
    "            mlflow_uri=\"https://dagshub.com/samaTech-sys/Rising-Village-Prediction-Model.mlflow\"\n",
    "        )\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a491fe",
   "metadata": {},
   "source": [
    "## Trial-runs for components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c58a39b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from raisingVillage import logger\n",
    "import mlflow\n",
    "from sklearn.metrics import (\n",
    "                            accuracy_score, \n",
    "                            precision_score, \n",
    "                            recall_score,\n",
    "                            f1_score, \n",
    "                            roc_auc_score, \n",
    "                            confusion_matrix,\n",
    "                            classification_report)\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b3846e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def eval_metrics(self, actual, pred, proba=None):\n",
    "        \"\"\"Calculate all evaluation metrics with proper type handling\"\"\"\n",
    "        # Convert predictions to binary if needed (threshold at 0.5)\n",
    "        if np.issubdtype(pred.dtype, np.floating):\n",
    "            pred = (pred >= 0.5).astype(int)\n",
    "        \n",
    "        # Ensure actual values are integers\n",
    "        actual = actual.astype(int)\n",
    "        \n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(actual, pred),\n",
    "            \"precision\": precision_score(actual, pred, zero_division=0),\n",
    "            \"recall\": recall_score(actual, pred, zero_division=0),\n",
    "            \"f1_score\": f1_score(actual, pred, zero_division=0),\n",
    "            \"confusion_matrix\": confusion_matrix(actual, pred).tolist(),\n",
    "            \"classification_report\": classification_report(actual, pred, output_dict=True, zero_division=0)\n",
    "        }\n",
    "        \n",
    "        if proba is not None:\n",
    "            metrics[\"roc_auc\"] = roc_auc_score(actual, proba)\n",
    "        return metrics\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        \"\"\"Main method to execute full evaluation workflow\"\"\"\n",
    "        try:\n",
    "            # Load data and model\n",
    "            test_data = pd.read_csv(str(self.config.test_data_path))\n",
    "            model = joblib.load(str(self.config.model_path))\n",
    "            \n",
    "            # Handle target column\n",
    "            target_column = \"target_binary\"\n",
    "            if target_column not in test_data.columns:\n",
    "                raise ValueError(f\"Target column '{target_column}' not found in test data\")\n",
    "            \n",
    "            # Prepare test data\n",
    "            test_x = test_data.drop([target_column], axis=1)\n",
    "            test_y = test_data[target_column].astype(int)  # Ensure binary target\n",
    "            \n",
    "            # Set MLflow tracking URI\n",
    "            mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "            \n",
    "            with mlflow.start_run():\n",
    "                # Get predictions and probabilities\n",
    "                predictions = model.predict(test_x)\n",
    "                probabilities = model.predict_proba(test_x)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = self.eval_metrics(test_y, predictions, probabilities)\n",
    "                \n",
    "                # Save and log metrics\n",
    "                self._save_metrics(metrics)\n",
    "                params_to_log = dict(self.config.all_params) if hasattr(self.config.all_params, 'to_dict') else self.config.all_params\n",
    "                mlflow.log_params(params_to_log)\n",
    "                \n",
    "                mlflow.log_metrics({\n",
    "                    \"accuracy\": metrics[\"accuracy\"],\n",
    "                    \"precision\": metrics[\"precision\"],\n",
    "                    \"recall\": metrics[\"recall\"],\n",
    "                    \"f1_score\": metrics[\"f1_score\"],\n",
    "                })\n",
    "                \n",
    "                if \"roc_auc\" in metrics:\n",
    "                    mlflow.log_metric(\"roc_auc\", metrics[\"roc_auc\"])\n",
    "                \n",
    "                # Model registry\n",
    "                tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "                if tracking_url_type_store != \"file\":\n",
    "                    mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"GradientBoostingModel\")\n",
    "                else:\n",
    "                    mlflow.sklearn.log_model(model, \"model\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during model evaluation: {str(e)}\")\n",
    "            raise RuntimeError(f\"Model evaluation failed: {str(e)}\") from e\n",
    "    \n",
    "    def _save_metrics(self, metrics):\n",
    "        \"\"\"Save metrics to JSON file\"\"\"\n",
    "        try:\n",
    "            self.config.root_dir.mkdir(parents=True, exist_ok=True)\n",
    "            metrics_path = self.config.root_dir / \"metrics.json\"\n",
    "            with open(metrics_path, \"w\") as f:\n",
    "                json.dump(metrics, f, indent=4)\n",
    "                \n",
    "            report_path = self.config.root_dir / \"classification_report.json\"\n",
    "            with open(report_path, \"w\") as f:\n",
    "                json.dump(metrics[\"classification_report\"], f, indent=4)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save metrics: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0145a5",
   "metadata": {},
   "source": [
    "## Trial-runs for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "decb9014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-29 19:09:15,286: INFO: common: yaml_file: config\\config.yaml loaded successfully]\n",
      "[2025-05-29 19:09:15,302: INFO: common: yaml_file: params.yaml loaded successfully]\n",
      "[2025-05-29 19:09:15,302: INFO: common: yaml_file: selected_schema.yaml loaded successfully]\n",
      "[2025-05-29 19:09:15,302: INFO: common: Created directory at: artifacts]\n",
      "[2025-05-29 19:09:15,318: INFO: common: Created directory at: artifacts/model_evaluation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'GradientBoostingModel'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-29 19:10:08,617: WARNING: connectionpool: Retrying (Retry(total=4, connect=5, read=4, redirect=5, status=5)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /samaTech-sys/Rising-Village-Prediction-Model.mlflow/api/2.0/mlflow/runs/get?run_uuid=e793ca1936e042d2986ebb3afa6edcc5&run_id=e793ca1936e042d2986ebb3afa6edcc5]\n",
      "[2025-05-29 19:10:34,512: WARNING: connectionpool: Retrying (Retry(total=3, connect=4, read=4, redirect=5, status=5)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E7C7DDB940>, 'Connection to dagshub.com timed out. (connect timeout=120)')': /samaTech-sys/Rising-Village-Prediction-Model.mlflow/api/2.0/mlflow/runs/get?run_uuid=e793ca1936e042d2986ebb3afa6edcc5&run_id=e793ca1936e042d2986ebb3afa6edcc5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/29 19:10:35 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: GradientBoostingModel, version 1\n",
      "Created version '1' of model 'GradientBoostingModel'.\n"
     ]
    }
   ],
   "source": [
    "#Update pipeline Step \n",
    "try: \n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config =config.get_model_evaluation_config()\n",
    "    model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluation_config.log_into_mlflow()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raisingVillage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
